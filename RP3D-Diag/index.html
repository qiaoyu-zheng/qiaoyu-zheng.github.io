<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

	<title>K-Diag: Knowledge-enhanced Disease Diagnosis in Radiographic Imaging</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:36px">Large-scale Long-tailed Disease Diagnosis on Radiology Images</span><br><br><br>
	</center>
  </table>
  
	<table align="center" width="900px">
            <tbody><tr>
                    <td align="center" width="180px">
              <center>
                <span style="font-size:16px"><a href="https://chaoyi-wu.github.io/">Chaoyi Wu<sup>*</sup></a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="180px">
              <center>
                <span style="font-size:16px"><a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang<sup>*</sup></a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="180px">
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1,2</sup></span>
                </center>
              </td>
              <td align="center" width="180px">
            
              
	      <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/">Yanfeng Wang</a><sup>1,2</sup></span>
                </center>
              </td>
              <td align="center" width="180px">
        <center>
              <span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2,<img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span>
            </td>
            <td align="center" width="180px">
            </center>
		        
          </tr>
        </tbody></table><br>
      

	  <table align="center" width="700px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>CMIC, Shanghai Jiao Tong University</span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai AI Laboratory</span>
                </center>
                </td>
        </tr></tbody></table>
        <table align="center" width="700px">
          <td align="center" width="200px">
            <center>
              <br>
              <span style="font-size:18px">
                Accepted by MICCAI-BTSD2023 (Workshop Oral)
              </span>
            </center>
          </td>
      </table>

	<table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href="https://github.com/MediaBrain-SJTU/K-Diag"> [GitHub]</a>
                  </span>
                </center>
              </td>

              <!-- <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="https://arxiv.org/pdf/2109.03230.pdf"> [arXiv]</a>
                  </span>
                </center>
              </td> -->

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Cite <a href="./cite.txt"> [BibTeX]</a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>
      <br><hr>
      <!-- <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      <left>
        In this paper, we consider the problem of enhancing self-supervised visual-language pre-training~(VLP) with medical-specific knowledge, 
        by exploiting the paired image-text reports from the radiological daily practice.
        In particular, we make the following contributions:
        <i>First</i>, unlike existing works that directly process the raw reports,
        we adopt a novel report pre-processing mechanism by simply extracting the useful medical entities, avoiding unnecessary complexity from understanding the language grammar;
        <i>Second</i>, we propose a novel entity embedding module by querying an external knowledge description base, to exploit the rich context of additional information that the medical domain affords, and implicitly build relationships between entities in the language embedding space;
        <i> Third</i>, we propose a novel Transformer-based fusion model for spatially aligning the entity description with visual signals at the image patch level only with self-supervised learning, thus enabling the ability for spatial grounding;
        <i> Fourth</i>, we conduct thorough experiments to validate the effectiveness of our proposed architecture, and benchmark on numerous public benchmarks {\em e.g.}, ChestX-ray14, 
        RSNA Pneumonia, SIIM-ACR Pneumothorax, COVIDx CXR-2, COVID Rural, and EdemaSeverity. 
        In both zero-shot and fine-tuning settings, our model has demonstrated strong performance compared with the former methods on disease classification and grounding.
      </center></p>
      <p><img class="center"  src="./resources/Method.png" width="800px"></p> -->

      <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
            <left>
              In this paper, we consider the problem of disease diagnosis. 
              Unlike the conventional learning paradigm that treats labels independently, we propose a knowledge-enhanced framework, that enables training visual representation with the guidance of medical domain knowledge. 
              In particular, we make the following contributions: 
              First, to explicitly incorporate experts' knowledge, we propose to learn a neural representation for the medical knowledge graph via contrastive learning, implicitly establishing relations between different medical concepts. 
              Second, while training the visual encoder, we keep the parameters of the knowledge encoder frozen and propose to learn a set of prompt vectors for efficient adaptation. 
              Third, we adopt a Transformer-based disease-query module for cross-model fusion, which naturally enables explainable diagnosis results via cross attention. 
              To validate the effectiveness of our proposed framework, we conduct thorough experiments on three x-ray imaging datasets across different anatomy structures, showing our model is able to exploit the implicit relations between diseases/findings, thus is beneficial to the commonly encountered problem in the medical domain, namely, long-tailed and zero-shot recognition, which conventional methods either struggle or completely fail to realize.
            </left></p>
          <p><img style="width:800px" src='./resources/teaser.png'></p>
      </div>
      <br><hr>
      <center> <h2> Architecture </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;"><left>
        Overview of the knowledge-enhanced disease diagnosis workflow. 
        The <i>knowledge encoder (left)</i> is first trained to learn a neural representation of the medical knowledge graph via contrastive learning, 
        and then used to guide the visual representation learning in our <i>knowledge-enhanced classification model (right)</i>.
        While training the visual encoder, we keep the parameters of the knowledge encoder frozen and propose to learn a set of prompt vectors for efficient adaptation.
        Finally we adopt a Transformer-based disease-query module for cross-model fusion, which naturally enables explainable diagnosis results via cross attention.
        
	</left></p>
        <p><img class="left"  src="./resources/Method.png" width="800px"></p>
      
      <br>
      <hr>
      <center><h2>Quantitative Results</h2></center>
      <p><b>R1: Analysis of Knowledge-Enhanced Classification Model </b> </p>
      <p><left>
        <i>Comparison to Conventional Training Scheme.</i> 
          Compare with Baseline Models with ResNet-50 and ViT-16 as backbone on disease classification tasks. 
          KE indicates the proposed knowledge encoder, LP indicates the proposed learnable prompt module, and the number denotes the prompt number. 
          AUC scores averaged across different diseases are reported. 
          Our knowledge-enhanced model achieves a higher average AUC on all datasets across different architectures.
      </left></p>
      <p><img class="center"  src="./resources/table1.png" width="800px"></p>
      
      <div class="container">
        <div class="image" width="550px">
          <center><p><img class="center"  src="./resources/table4.png" width="500px"></p></center>
	      </div>
        <div class="text" width="250px"> 
          <p> Compare with Baseline Models on VinDr-Mammo Task. AUC scores are reported. 
            Detail results for each diseases are shown in the table to explain that, on VinDr-Mammo, LP sometimes may have a lower scores mainly because of some classes with extremly few cases.
          </p>
        </div>
      </div>
      <p><b>R2: Analysis of the Knowledge-Enhanced Text Encoder </b> </p>	
      <p> Ablation study on knowledge encoder with ResNet as a backbone, we use the optimal prompt numbers according to the ablation study, 
        i.e., 32 for VinDr-PCXR, 128 for VinDr-Mammo, and 64 for VinDr-SpineXr.
        we can make two observations: <i>(i)</i> guiding visual representation learning with domain knowledge
        generally works better, e.g., results of using ClinicalBERT or PubMedBERT outperform
        conventional training with discrete labels, <i>(ii)</i> our proposed knowledge-enhanced text
        encoder consistently demonstrates superior results, that can be attributed to the explicitly
        injected domain knowledge, rather than implicitly learning it from the document corpus.
          </p>
			<img style="width:800px" src='./resources/table2.png'></img>
			
      
        
      <p><b>R3: Analysis on the CXR-Mix</b></p>
      <p> <b>The Ability to Combine Various Partial-labeled Datasets:</b>
        Compare with Baseline Models on disease classification tasks on the assembling dataset.
        AvgAUC refers to the AUC score averaged across different diseases. The first line refers to the use
        of the training flow proposed by TorchXrayVision and use ResNet or ViT as the backbone.
        Unlike the traditional approach, which requires to carefully merging the label space from different datasets
        to benefit from them, our formulation of embedding the 'disease name' with a knowledge
        encoder naturally enables us to train models on the mixture of multiple datasets, 
        handling different granularities of diagnosis targets and inconsistent pathology expression.
          </p>
      <center><p><img class="center"  src="./resources/table3.png" width="750px"></p></center>
      <p> <b>The Ability to Leverage Class Diversity:</b> 
        Analyse the performance gain on the assembling dataset. "Seperation" refers to using a
        single dataset to train our framework. "+Diversity" refers to adding the cases beyond the target
        classes, increasing the class diversity, and keeping the data amount of the target classes constant.
        "+Diversity+Amount" means directly mixing the 11 datasets and for most datasets, the data
        amount of the target classes will further increase.
          </p>
      <center><p><img class="center"  src="./resources/barplot.png" width="800px"></p></center>
      
      <p> <b>The Ability to Diagnose Open-set Unseen Diseases:</b> 
        AUC and 95% CI are shown on the unseen classes under the zero-shot setting. n represents
        the number of related cases. Generally, our method achieves an AUC of at least 0.800 on 14
        findings, at least 0.700 on 45 findings and at least 0.600 on 79 findings out of 106 radiographic findings where n > 50 in the
        PadChest test dataset (n = 39, 053).
          </p>
      <center><p><img class="center"  src="./resources/padchest.png" width="800px"></p></center>

      <br>
      <hr>

      <center><h2>Visualizations of Zero-shot Grounding </h2></center>
      <!-- <p><b>2D Visualization </b> </p> -->
      <p><left>
        We average the cross-attention map in each transformer layer in the disease query module, 
        and visualize the results in Figure. The model's attention well matches radiologists'
        diagnoses of different diseases, i.e. red boxes labeled by radiologists.
      </left></p>
      <p><img class="center"  src="./resources/visualize.png" width="800px"></p>
      <br>
      <hr>
      <center> <h2> Acknowledgements </h2> </center>
      <p> 
	      Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
      </p>
      <br>
<br>
</body>
</html>
