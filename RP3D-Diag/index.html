<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--
  <script src="./resources/jsapi" type="text/javascript"></script>
  <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
 -->

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  .right-align {
    text-align: right;
  }

  figure {
    text-align: center;
  }

  figcaption {
    font-style: italic;
    margin-top: 5px;
  }
</style>

	<title>Large-scale Long-tailed Disease Diagnosis on Radiology Images</title>
</head>

<body>
	<br>
	<center>
	<span style="font-size:36px">Large-scale Long-tailed Disease Diagnosis on Radiology Images</span><br><br><br>
	</center>
  </table>
  
	<table align="center" width="650px">
            <tbody><tr>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://qiaoyu-zheng.github.io/">Qiaoyu Zheng<sup>*</sup></a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://github.com/Angelakeke/">Weike Zhao<sup>*</sup></a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://chaoyi-wu.github.io/">Chaoyi Wu<sup>*</sup></a><sup>1,2</sup></span>
                </center>
                </td>
                    <td align="center" width="160px">
              <center>
                <span style="font-size:16px"><a href="https://xiaoman-zhang.github.io/">Xiaoman Zhang</a><sup>1,2</sup></span>
                </center>
                </td>
                
		        
          </tr>
        </tbody></table><br>

    <table align="center" width="500px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                <td align="center" width="300px">
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/yazhang/">Ya Zhang</a><sup>1,2</sup></span>
                </center>
              </td>
                    <td align="center" width="300px">
              <center>
                <span style="font-size:16px"><a href="https://mediabrain.sjtu.edu.cn/">Yanfeng Wang</a><sup>1,2,<img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span>
                </center>
              </td>
              <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1,2,<img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span>
                  </td>
                  
        </tr></tbody></table><br>
      

	  <table align="center" width="700px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>CMIC, Shanghai Jiao Tong University</span>
                </center>
                </td>
                    <td align="center" width="300px">
              <center>
                    <span style="font-size:16px"><sup>2</sup>Shanghai AI Laboratory</span>
                </center>
                </td>
        </tr></tbody></table>
        

	<table align="center" width="700px">
            <tbody><tr>
              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">Code
                    <a href="https://github.com/qiaoyu-zheng/RP3D-Diag"> [GitHub]</a>
                  </span>
                </center>
              </td>

              <!-- <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="https://arxiv.org/pdf/2109.03230.pdf"> [arXiv]</a>
                  </span>
                </center>
              </td> -->

              <td align="center" width="200px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Paper <a href="https://arxiv.org/abs/2312.16151"> [arXiv]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="240px">
                <center>
                  <br>
                  <span style="font-size:20px">
                    Model <a href="https://huggingface.co/QiaoyuZheng/RadDiag"> [HuggingFace]</a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>
     
      <br><hr>
      <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
            <left>
              Developing a generalist radiology diagnosis system can greatly enhance clinical diagnostics. In this paper,
              we introduce RadDiag, a foundational model supporting 2D and 3D inputs across various modalities and
              anatomies, using a transformer-based fusion module for comprehensive disease diagnosis. Due to patient
              privacy concerns and the lack of large-scale radiology diagnosis datasets, we utilize high-quality, clinician-
              reviewed radiological images available online with diagnosis labels. Our dataset, RP3D-DiagDS, contains
              40,936 cases with 195,010 scans covering 5,568 disorders (930 unique ICD-10-CM codes). Experimentally,
              our RadDiag achieves 95.14% AUC on internal evaluation with the knowledge-enhancement strategy.
              Additionally, RadDiag can be zero-shot applied or fine-tuned to external diagnosis datasets sourced from
              various hospitals, demonstrating state-of-the-art results. In conclusion, we show that publicly shared
              medical data on the Internet is a tremendous and valuable resource that can potentially support building a
              generalist AI for healthcare. </left></p>
             
      </div>
      <br>

      <hr>
      <center> <h2> The RP3D-DiagDS Dataset </h2> </center>

 <p><img style="width:800px" src='./resources/RP3D-DiagDS.png'></p>
      <p>
Overall, the proposed dataset contains 39,026 cases, 
of 192,675 images from 9 diverse imaging modalities and 7 human anatomy regions, 
note that, each case may contain images of multiple scans. 
The data covers 5,568 different disorders, that have been manually mapped into 930 <a href="https://www.icd10data.com/ICD10CM/Codes">ICD-10-CM</a> codes. 
      </p>
 <!-- <center><p><img style="width:500px" src='./resources/fig2.png'></p></center> -->
      <p>
        Specifically, cases in our dataset are sourced from the <a href="https://radiopaedia.org/search?scope=cases">Radiopaedia website</a>
        -- a growing peer-reviewed educational radiology resource website, that allows the clinicians to upload 3D volumes to better reflect real clinical scenarios. 
        Additionally, all privacy issues have already been resolved by the clinicians at uploading time. 
      </p>
      <center><p><img style="width:600px" src='./resources/filter_procedure.png'></p></center>

      <p>
        For each cases in <a href="https://radiopaedia.org/search?scope=cases">Radiopaedia</a>, 'Related Radiopaedia articles' contains links to related articles named with corresponding disorders for radiology images, 
        which are treated as diagnosis labels and have been meticulously peer-reviewed by experts in Radiopaedia Editorial Board.
      </p>
      <p>
        After article filtering, manual mapping and normal cases adding, we get 39,026 cases containing 192,675 images labeled by 5,568 disorder classes and 930 ICD-10-CM classes.
        We will continually maintain the dataset, growing the case number.
      </p>

      <p style="font-weight: bold; font-size: large;">
	      Analysis of the Cases in RP3D-DiagDS dataset
      </p>
        
      <p> 
	      RP3D-DiagDS comprises images from 9 modalities, namely, 
        <strong>computed tomography (CT)</strong>, <strong>magnetic resonance imaging (MRI)</strong>, 
        <strong>X-ray</strong>, <strong>Ultrasound</strong>, <strong>Fluoroscopy</strong>, <strong>Nuclear medicine</strong>, <strong>Mammography</strong>, 
        <strong>DSA (angiography)</strong>, and <strong>Barium Enema</strong>. Each case may include images from multiple modalities, to ensure precise and comprehensive diagnosis of disorders. 
        Overall, approximately 19.4% of the cases comprise images from two modalities, while around 2.9% involve images from three to five modalities. 
        The remaining cases are associated with image scans from a single modality. 
      </p>
      <p><img style="width:800px" src='./resources/Distribution_figure.png'></p>
      <!-- <iframe src="./resources/modality_good1.pdf" width="800px" height="100%"></iframe> -->
      <!-- <embed src="./resources/modality_good1.pdf" type="application/pdf" width="100%" height="600px" /> -->
      <!-- <p> 
	      RP3D-DiagDS comprises images from various anatomical regions, including <strong>head and neck</strong>, <strong>spine</strong>, <strong>chest</strong>, <strong>breast</strong>, 
        <strong>abdomen and pelvis</strong>, <strong>upper limb</strong>, and <strong>lower limb</strong>, providing comprehensive coverage of the entire human body. 
      </p>
      <p><img style="width:800px" src='./resources/anatomy_good.png'></p> -->
      
      <p>
        RP3D-DiagDS comprises images from various anatomical regions,
        including the head and neck, spine, chest, breast, abdomen and pelvis, upper limb, and lower limb, providing
        comprehensive coverage of the entire human body.
        Totally there are 5568 disorders mapped into 930 ICD-10-CM
        classes. We define the “head class” category with case counts greater than 100, the “body class” category with
        case counts between 30 and 100, and the “tail class” category with case counts less than 30.
      </p>
      <!-- <center><p><img style="width:600px" src='./resources/fig4.png'></p></center> -->

      <br>

      <hr>
      <center> <h2> Architecture </h2> </center>
      <p>
      <figure>
        <img style="width:700px" src='./resources/RP3D-DiagModel.png'>
        <figcaption>The architecture of our proposed visual encoder and fusion module, together with the knowledge enhancement strategy. (a) shows the details of the vision encoder. We design two variants to fit in the two main visual backbones, i.e., ResNet and ViT. (b) shows the transformer-based fusion module, enabling case-level information fusion. (c) shows the knowledge enhancement strategy. We first pre-train a text encoder with extra medical knowledge with contrastive learning, i.e., synonyms, descriptions and hierarchy, termed as knowledge encoder and then we view the text embedding as a natural classifier to guide the diagnosis classification.</figcaption>
      </figure>
    </p>

    <br>

    <hr>
    <center> <h2> Results </h2> </center>

    <p style="font-weight: bold; font-size: large;">
      R1: Classification results on Disorders and ICD-10-CM levels. 
    </p>
    <p><center><img style="width:700px" src='./resources/table1.jpeg'></center></p>
    <p>
      In the table ``FM'' represents the fusion module and ``KE'' represents the knowledge enhancement strategy. We report the results on Head/Medium/Tail class sets separately.
    </p>
    <!-- <br> -->
    <p style="font-weight: bold; font-size: large;">
      R2: ROC curves on Disorders and ICD-10-CM.
    </p>
    <p><center><img style="width:700px" src='./resources/table2.jpeg'></center></p>
    <p>
      As depicts in ROC curves above, the shadow in the figure shown the 95% CI (Confidence interval) and FM, KE are short for Fusion Module and Knowledge Enhancement.
    </p>

    <!-- <br> -->
    <p style="font-weight: bold; font-size: large;">
      R3: The AUC Score Comparison on Various External Datasets.
    </p>
    <p><center><img style="width:700px" src='./resources/table3.jpeg'></center></p>
    <p>
      For each dataset, we carry out experiments with different training data portions, denoted as 1% to 100% in the table. 
      For example, 30% represents we use 30% of data in the downstream training set for finetuning our model or training from scratch. 
      ``SOTA'' denotes the best performance of former works (pointed with corresponding reference) on the datasets and 
      ``Zero-shot'' denotes directly evaluate our model on external datasets. 
      We mark the gap between ours and training from screatch on the subscript of uparrows in the table.
    </p>

    <p>For more detailed ablation studies and results, please refer to our <a href="https://arxiv.org/abs/2312.16151"> paper</a>.</p>

    <br>
      <hr>
      <center><h2>BibTeX</h2></center>
      <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
        @article{zheng2023large,
          title={Large-scale Long-tailed Disease Diagnosis on Radiology Images},
          author={Zheng, Qiaoyu and Zhao, Weike and Wu, Chaoyi and Zhang, Xiaoman and Zhang, 
            Ya and Wang, Yanfeng and Xie, Weidi},
          journal={arXiv preprint arXiv:2312.16151},
          year={2023}
        }
              </code>
          </pre>
        </div>      
      </div>

      
      <br>
<br>
</body>
</html>
